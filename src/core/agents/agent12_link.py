"""
Agent 12: Link - Communications Bridge & Integration Controller

In the Matrix, Link is the vital communications interface between the real world
and the Matrix. He ensures seamless data flow and system interoperability, 
specially focusing on the critical Agent 1 â†’ Agent 9 data flow for import table
reconstruction and MFC 7.1 compatibility.

Matrix Context:
Link's role as the communications expert translates to advanced data flow validation
and integration control, with special focus on ensuring Agent 1's rich import analysis
reaches Agent 9 for proper resource compilation.

CRITICAL MISSION: Validate Agent 1 â†’ Agent 9 data flow for import table reconstruction,
ensure MFC 7.1 compatibility data reaches compilation, and provide fail-fast validation
for insufficient data quality.

Production-ready implementation following SOLID principles and NSA-level security standards.
Includes focused integration validation and Agent 1/9 data flow monitoring.
"""

import logging
import time
import json
import hashlib
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed

# Matrix framework imports
try:
    from ..matrix_agents import ReconstructionAgent, MatrixCharacter, AgentStatus
    from ..shared_components import (
        MatrixLogger, MatrixFileManager, MatrixValidator, MatrixProgressTracker, 
        MatrixErrorHandler, MatrixMetrics, SharedAnalysisTools, SharedValidationTools
    )
    from ..exceptions import MatrixAgentError, ValidationError, ConfigurationError
    HAS_MATRIX_FRAMEWORK = True
except ImportError:
    # Fallback for basic execution
    HAS_MATRIX_FRAMEWORK = False

@dataclass
class IntegrationResult:
    """Result of integration and communication bridge process"""
    success: bool = False
    data_integrity_score: float = 0.0
    communication_quality: float = 0.0
    integration_completeness: float = 0.0
    bridge_status: Dict[str, bool] = field(default_factory=dict)
    data_flows: Dict[str, Any] = field(default_factory=dict)
    validation_results: Dict[str, Any] = field(default_factory=dict)
    communication_logs: List[str] = field(default_factory=list)
    error_messages: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)

@dataclass
class CommunicationChannel:
    """Communication channel configuration"""
    name: str
    source: str
    destination: str
    protocol: str = "direct"
    validation_enabled: bool = True
    compression_enabled: bool = False
    encryption_enabled: bool = False
    retry_count: int = 3
    timeout: float = 30.0
    data_format: str = "json"

class Agent12_Link(ReconstructionAgent):
    """
    Agent 12: Link - Communications Bridge & Integration Controller
    
    The communications expert who ensures critical data flows between agents,
    with special focus on the Agent 1 â†’ Agent 9 import table data flow that
    addresses the primary bottleneck (64.3% discrepancy: 538â†’5 DLLs).
    
    Features:
    - Agent 1 â†’ Agent 9 data flow validation for import table reconstruction
    - MFC 7.1 compatibility data transfer monitoring
    - Fail-fast validation for insufficient import data quality
    - Core integration validation between critical pipeline components
    - Communication bridge management for essential data flows
    - Performance-optimized data transfer validation
    """
    
    def __init__(self):
        super().__init__(
            agent_id=12,
            matrix_character=MatrixCharacter.LINK
        )
        
        # Core components
        self.logger = self._setup_logging()
        if HAS_MATRIX_FRAMEWORK:
            # File manager will be initialized with proper output paths from context in execute()
            self.file_manager = None
        else:
            self.file_manager = None
        self.validator = MatrixValidator() if HAS_MATRIX_FRAMEWORK else None
        self.progress_tracker = MatrixProgressTracker(5, "Link") if HAS_MATRIX_FRAMEWORK else None
        self.error_handler = MatrixErrorHandler("Link") if HAS_MATRIX_FRAMEWORK else None
        
        # Communication components
        self.communication_channels = self._initialize_communication_channels()
        self.data_validators = self._load_data_validators()
        self.integration_protocols = self._load_integration_protocols()
        
        # State tracking
        self.current_phase = "initialization"
        self.active_bridges = {}
        self.data_checksums = {}
        self.communication_stats = {}
        
    def _setup_logging(self) -> logging.Logger:
        """Setup agent-specific logging"""
        logger = logging.getLogger(f"Matrix.Link")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '[Link] %(asctime)s - %(levelname)s - %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def get_dependencies(self) -> List[int]:
        """Get list of required predecessor agents"""
        return [11]  # The Oracle
    
    def get_description(self) -> str:
        """Get agent description"""
        return ("Link serves as the communications bridge and integration controller, "
                "ensuring seamless data flow and system interoperability across all pipeline components.")

    def execute_matrix_task(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute communications bridge and integration control"""
        self.logger.info("ðŸ”— Link establishing communication bridges...")
        
        # Initialize file manager with proper output paths from context
        if HAS_MATRIX_FRAMEWORK and 'output_paths' in context:
            self.file_manager = MatrixFileManager(context['output_paths'])
        
        start_time = time.time()
        
        try:
            # Phase 1: Validate dependencies and establish communications
            self.current_phase = "communication_setup"
            self.logger.info("Phase 1: Establishing communication protocols...")
            comm_setup_result = self._setup_communications(context)
            
            if not comm_setup_result['success']:
                return self._create_failure_result(
                    f"Communication setup failed: {comm_setup_result['error']}", 
                    start_time
                )
            
            # Phase 2: Validate data integrity across all components
            self.current_phase = "data_validation"
            self.logger.info("Phase 2: Validating data integrity...")
            integrity_result = self._validate_data_integrity(context)
            
            # Phase 3: Perform cross-reference analysis
            self.current_phase = "cross_reference_analysis"
            self.logger.info("Phase 3: Performing cross-reference analysis...")
            cross_ref_result = self._perform_cross_reference_analysis(context)
            
            # Phase 4: Critical Agent 1 â†’ Agent 9 data flow validation
            self.current_phase = "agent1_agent9_validation"
            self.logger.info("Phase 4: Validating Agent 1 â†’ Agent 9 data flow...")
            agent1_9_result = self._validate_agent1_agent9_dataflow(context)
            
            # Phase 5: Execute core integration validation
            self.current_phase = "integration_validation"
            self.logger.info("Phase 5: Validating core system integration...")
            integration_result = self._validate_core_integration(context)
            
            # Phase 6: Generate integration report
            self.current_phase = "report_generation"
            self.logger.info("Phase 6: Generating integration report...")
            final_result = self._generate_integration_report(
                integrity_result, cross_ref_result, agent1_9_result, integration_result, context
            )
            
            execution_time = time.time() - start_time
            
            self.logger.info(f"ðŸŽ¯ Link integration completed in {execution_time:.2f}s")
            self.logger.info(f"âœ… Integration Success: {final_result.success}")
            self.logger.info(f"ðŸ“Š Data Integrity: {final_result.data_integrity_score:.2f}")
            self.logger.info(f"ðŸ”— Communication Quality: {final_result.communication_quality:.2f}")
            self.logger.info(f"ðŸŽ›ï¸ Integration Completeness: {final_result.integration_completeness:.2f}")
            
            # Return dict from execute_matrix_task - base class will wrap in AgentResult
            return {
                'integration_result': final_result,
                'data_integrity_score': final_result.data_integrity_score,
                'communication_quality': final_result.communication_quality,
                'integration_completeness': final_result.integration_completeness,
                'bridge_status': final_result.bridge_status,
                'data_flows': final_result.data_flows,
                'validation_results': final_result.validation_results,
                'communication_stats': self.communication_stats,
                'active_bridges': len(self.active_bridges),
                'communication_channels': len(self.communication_channels),
                'warnings': len(final_result.warnings),
                'errors': len(final_result.error_messages),
                'critical_dataflow_validation': {
                    'agent1_agent9_validated': final_result.data_flows.get('agent1_agent9_flow', {}).get('validated', False),
                    'import_table_data_quality': final_result.data_flows.get('agent1_agent9_flow', {}).get('import_data_quality', 0.0),
                    'mfc71_compatibility_validated': final_result.data_flows.get('agent1_agent9_flow', {}).get('mfc71_compatible', False),
                    'dll_count_accuracy': final_result.data_flows.get('agent1_agent9_flow', {}).get('dll_count_match', False)
                }
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"Link integration failed in {self.current_phase}: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            
            # Re-raise exception - base class will handle creating AgentResult
            raise Exception(error_msg) from e
    
    def _setup_communications(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Setup communication channels and validate Oracle dependency"""
        try:
            agent_results = context.get('agent_results', {})
            
            # Check The Oracle (Agent 11) result
            if 11 not in agent_results:
                return {'success': False, 'error': 'The Oracle (Agent 11) result not available'}
            
            oracle_result = agent_results[11]
            status = oracle_result.get('status', 'unknown')
            
            if status != 'success' and status != AgentStatus.SUCCESS:
                return {'success': False, 'error': 'The Oracle validation failed'}
            
            # Establish communication bridges with all previous agents
            for agent_id in range(1, 12):
                if agent_id in agent_results:
                    bridge_name = f"agent_{agent_id}_bridge"
                    self.active_bridges[bridge_name] = {
                        'agent_id': agent_id,
                        'status': 'active',
                        'data_available': True,
                        'last_communication': time.time()
                    }
            
            self.logger.info(f"Established {len(self.active_bridges)} communication bridges")
            return {'success': True, 'error': None}
            
        except Exception as e:
            return {'success': False, 'error': f'Communication setup failed: {str(e)}'}
    
    def _validate_data_integrity(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Validate data integrity across all pipeline components"""
        integrity_result = {
            'overall_integrity': 0.0,
            'component_integrity': {},
            'checksum_validation': {},
            'data_consistency_checks': {},
            'issues': [],
            'validated_components': 0
        }
        
        try:
            agent_results = context.get('agent_results', {})
            
            # Validate each agent's data integrity
            integrity_scores = []
            
            for agent_id, result in agent_results.items():
                component_name = f"agent_{agent_id}"
                
                # Calculate data integrity score
                data_score = self._calculate_component_integrity(agent_id, result)
                integrity_scores.append(data_score)
                integrity_result['component_integrity'][component_name] = data_score
                
                # Generate and validate checksums
                checksum = self._generate_data_checksum(result)
                self.data_checksums[component_name] = checksum
                integrity_result['checksum_validation'][component_name] = True
                
                # Perform consistency checks
                consistency_check = self._check_data_consistency(agent_id, result, context)
                integrity_result['data_consistency_checks'][component_name] = consistency_check
                
                integrity_result['validated_components'] += 1
            
            # Calculate overall integrity
            if integrity_scores:
                integrity_result['overall_integrity'] = sum(integrity_scores) / len(integrity_scores)
            
            # Identify integrity issues
            low_integrity_components = [
                comp for comp, score in integrity_result['component_integrity'].items() 
                if score < 0.7
            ]
            
            if low_integrity_components:
                integrity_result['issues'].extend([
                    f"Low integrity detected in {comp}: {integrity_result['component_integrity'][comp]:.2f}"
                    for comp in low_integrity_components
                ])
            
        except Exception as e:
            integrity_result['issues'].append(f"Data integrity validation error: {str(e)}")
        
        return integrity_result
    
    def _perform_cross_reference_analysis(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Perform cross-reference and linking analysis"""
        cross_ref_result = {
            'reference_map': {},
            'linking_analysis': {},
            'dependency_validation': {},
            'cross_component_references': {},
            'reference_integrity': 0.0,
            'unresolved_references': [],
            'circular_dependencies': []
        }
        
        try:
            agent_results = context.get('agent_results', {})
            
            # Build comprehensive reference map
            reference_map = {}
            
            # Extract references from each component
            for agent_id, result in agent_results.items():
                component_refs = self._extract_component_references(agent_id, result)
                reference_map[f"agent_{agent_id}"] = component_refs
            
            cross_ref_result['reference_map'] = reference_map
            
            # Analyze cross-component references
            cross_component_refs = self._analyze_cross_component_references(reference_map)
            cross_ref_result['cross_component_references'] = cross_component_refs
            
            # Validate reference integrity
            reference_integrity = self._validate_reference_integrity(reference_map, cross_component_refs)
            cross_ref_result['reference_integrity'] = reference_integrity
            
            # Detect unresolved references
            unresolved = self._detect_unresolved_references(reference_map)
            cross_ref_result['unresolved_references'] = unresolved
            
            # Check for circular dependencies
            circular_deps = self._detect_circular_dependencies(reference_map)
            cross_ref_result['circular_dependencies'] = circular_deps
            
            # Generate linking analysis
            linking_analysis = self._perform_linking_analysis(reference_map, context)
            cross_ref_result['linking_analysis'] = linking_analysis
            
        except Exception as e:
            cross_ref_result['unresolved_references'].append(f"Cross-reference analysis error: {str(e)}")
        
        return cross_ref_result
    
    def _validate_agent1_agent9_dataflow(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        CRITICAL: Validate Agent 1 â†’ Agent 9 data flow for import table reconstruction
        
        This addresses the core bottleneck where Agent 1's rich import analysis (538 functions 
        from 14 DLLs) must reach Agent 9 for proper resource compilation, but currently
        fails resulting in only 5 basic DLLs being included.
        """
        dataflow_result = {
            'validated': False,
            'import_data_quality': 0.0,
            'mfc71_compatible': False,
            'dll_count_match': False,
            'agent1_data_available': False,
            'agent9_data_received': False,
            'dataflow_integrity': 0.0,
            'critical_issues': [],
            'compatibility_analysis': {}
        }
        
        try:
            agent_results = context.get('agent_results', {})
            
            # Check Agent 1 (Sentinel) import analysis availability
            if 1 in agent_results:
                agent1_data = agent_results[1]
                dataflow_result['agent1_data_available'] = True
                
                # Extract import table data from Agent 1
                import_analysis = self._get_agent_data_safely(agent1_data, 'import_analysis')
                dll_analysis = self._get_agent_data_safely(agent1_data, 'dll_analysis')
                
                if import_analysis and dll_analysis:
                    # Analyze import data quality
                    import_count = len(import_analysis.get('imported_functions', []))
                    dll_count = len(dll_analysis.get('required_dlls', []))
                    
                    # Check for MFC 7.1 compatibility indicators
                    mfc_dlls = [dll for dll in dll_analysis.get('required_dlls', []) 
                               if 'mfc' in dll.lower() and '71' in dll]
                    dataflow_result['mfc71_compatible'] = len(mfc_dlls) > 0
                    
                    # Calculate import data quality (target: 538 functions, 14 DLLs)
                    function_ratio = min(import_count / 538.0, 1.0) if import_count else 0.0
                    dll_ratio = min(dll_count / 14.0, 1.0) if dll_count else 0.0
                    dataflow_result['import_data_quality'] = (function_ratio + dll_ratio) / 2.0
                    
                    # Check if DLL count matches expectations (should be close to 14)
                    dataflow_result['dll_count_match'] = dll_count >= 10  # Allow some variance
                    
                    # Store compatibility analysis
                    dataflow_result['compatibility_analysis'] = {
                        'import_functions_found': import_count,
                        'target_functions': 538,
                        'dlls_found': dll_count,
                        'target_dlls': 14,
                        'mfc71_dlls_detected': len(mfc_dlls),
                        'vs2022_compatibility_issues': dll_count < 10
                    }
                    
                else:
                    dataflow_result['critical_issues'].append("Agent 1 import analysis data incomplete")
            else:
                dataflow_result['critical_issues'].append("Agent 1 (Sentinel) results not available")
            
            # Check Agent 9 (The Machine) data reception
            if 9 in agent_results:
                agent9_data = agent_results[9]
                resource_compilation = self._get_agent_data_safely(agent9_data, 'resource_compilation')
                build_dependencies = self._get_agent_data_safely(agent9_data, 'build_dependencies')
                
                if resource_compilation and build_dependencies:
                    dataflow_result['agent9_data_received'] = True
                    
                    # Check if Agent 9 received and processed Agent 1's import data
                    compiled_dlls = build_dependencies.get('required_dlls', [])
                    if len(compiled_dlls) >= 10:  # Should include most of the 14 DLLs
                        dataflow_result['dataflow_integrity'] = 0.8
                    elif len(compiled_dlls) >= 5:
                        dataflow_result['dataflow_integrity'] = 0.4
                        dataflow_result['critical_issues'].append("Agent 9 only received basic DLL set (5), missing rich import data")
                    else:
                        dataflow_result['dataflow_integrity'] = 0.1
                        dataflow_result['critical_issues'].append("Agent 9 received insufficient DLL dependencies")
                else:
                    dataflow_result['critical_issues'].append("Agent 9 resource compilation data incomplete")
            else:
                dataflow_result['critical_issues'].append("Agent 9 (The Machine) results not available")
            
            # Overall validation - both agents must have data and quality must be sufficient
            if (dataflow_result['agent1_data_available'] and 
                dataflow_result['agent9_data_received'] and 
                dataflow_result['import_data_quality'] > 0.6 and
                dataflow_result['dataflow_integrity'] > 0.6):
                dataflow_result['validated'] = True
            else:
                # Fail-fast validation following rules.md
                if dataflow_result['import_data_quality'] < 0.6:
                    dataflow_result['critical_issues'].append("FAIL-FAST: Import data quality insufficient for reconstruction")
                if dataflow_result['dataflow_integrity'] < 0.6:
                    dataflow_result['critical_issues'].append("FAIL-FAST: Agent 1â†’9 data flow integrity insufficient")
                
        except Exception as e:
            dataflow_result['critical_issues'].append(f"Agent 1â†’9 validation error: {str(e)}")
        
        return dataflow_result
    
    def _get_agent_data_safely(self, agent_data: Any, key: str) -> Any:
        """Safely get data from agent result, handling both dict and AgentResult objects"""
        if hasattr(agent_data, 'data') and hasattr(agent_data.data, 'get'):
            return agent_data.data.get(key)
        elif hasattr(agent_data, 'get'):
            data = agent_data.get('data', {})
            if hasattr(data, 'get'):
                return data.get(key)
        return None
    
    def _validate_core_integration(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Validate overall system integration"""
        integration_result = {
            'integration_score': 0.0,
            'component_compatibility': {},
            'data_flow_validation': {},
            'protocol_compliance': {},
            'interface_validation': {},
            'integration_issues': [],
            'compatibility_matrix': {}
        }
        
        try:
            agent_results = context.get('agent_results', {})
            
            # Validate component compatibility
            compatibility_scores = []
            component_pairs = []
            
            agents = list(agent_results.keys())
            for i, agent_a in enumerate(agents):
                for agent_b in agents[i+1:]:
                    compatibility_score = self._check_component_compatibility(
                        agent_a, agent_b, agent_results[agent_a], agent_results[agent_b]
                    )
                    
                    pair_name = f"agent_{agent_a}_agent_{agent_b}"
                    component_pairs.append(pair_name)
                    integration_result['component_compatibility'][pair_name] = compatibility_score
                    compatibility_scores.append(compatibility_score)
            
            # Calculate overall integration score
            if compatibility_scores:
                integration_result['integration_score'] = sum(compatibility_scores) / len(compatibility_scores)
            
            # Validate data flow paths
            data_flow_validation = self._validate_data_flows(agent_results, context)
            integration_result['data_flow_validation'] = data_flow_validation
            
            # Check protocol compliance
            protocol_compliance = self._check_protocol_compliance(agent_results)
            integration_result['protocol_compliance'] = protocol_compliance
            
            # Validate interfaces
            interface_validation = self._validate_component_interfaces(agent_results)
            integration_result['interface_validation'] = interface_validation
            
            # Generate compatibility matrix
            integration_result['compatibility_matrix'] = self._generate_compatibility_matrix(agents, agent_results)
            
            # Identify integration issues
            low_compatibility_pairs = [
                pair for pair, score in integration_result['component_compatibility'].items() 
                if score < 0.6
            ]
            
            if low_compatibility_pairs:
                integration_result['integration_issues'].extend([
                    f"Low compatibility: {pair} ({integration_result['component_compatibility'][pair]:.2f})"
                    for pair in low_compatibility_pairs
                ])
            
        except Exception as e:
            integration_result['integration_issues'].append(f"System integration validation error: {str(e)}")
        
        return integration_result
    
    def _check_system_interoperability(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Check system interoperability and cross-platform compatibility"""
        interop_result = {
            'interoperability_score': 0.0,
            'platform_compatibility': {},
            'api_compatibility': {},
            'data_format_compatibility': {},
            'version_compatibility': {},
            'interoperability_issues': [],
            'supported_platforms': []
        }
        
        try:
            agent_results = context.get('agent_results', {})
            binary_info = context.get('binary_info', {})
            
            # Check platform compatibility
            target_platforms = ['windows', 'linux', 'macos']
            platform_scores = []
            
            for platform in target_platforms:
                platform_score = self._check_platform_compatibility(platform, agent_results, binary_info)
                interop_result['platform_compatibility'][platform] = platform_score
                platform_scores.append(platform_score)
                
                if platform_score > 0.7:
                    interop_result['supported_platforms'].append(platform)
            
            # Check API compatibility
            api_compatibility = self._check_api_compatibility(agent_results)
            interop_result['api_compatibility'] = api_compatibility
            
            # Check data format compatibility
            data_format_compatibility = self._check_data_format_compatibility(agent_results)
            interop_result['data_format_compatibility'] = data_format_compatibility
            
            # Check version compatibility
            version_compatibility = self._check_version_compatibility(agent_results)
            interop_result['version_compatibility'] = version_compatibility
            
            # Calculate overall interoperability score
            if platform_scores:
                interop_result['interoperability_score'] = sum(platform_scores) / len(platform_scores)
            
            # Identify interoperability issues
            unsupported_platforms = [
                platform for platform, score in interop_result['platform_compatibility'].items() 
                if score < 0.5
            ]
            
            if unsupported_platforms:
                interop_result['interoperability_issues'].extend([
                    f"Limited support for {platform}: {interop_result['platform_compatibility'][platform]:.2f}"
                    for platform in unsupported_platforms
                ])
            
        except Exception as e:
            interop_result['interoperability_issues'].append(f"Interoperability check error: {str(e)}")
        
        return interop_result
    
    def _generate_integration_report(self, integrity_result: Dict[str, Any], 
                                   cross_ref_result: Dict[str, Any],
                                   agent1_9_result: Dict[str, Any],
                                   integration_result: Dict[str, Any],
                                   context: Dict[str, Any]) -> IntegrationResult:
        """Generate comprehensive integration report"""
        final_result = IntegrationResult()
        
        try:
            # Calculate overall scores
            final_result.data_integrity_score = integrity_result.get('overall_integrity', 0.0)
            final_result.communication_quality = self._calculate_communication_quality()
            final_result.integration_completeness = integration_result.get('integration_score', 0.0)
            
            # Determine overall success - CRITICAL: Include Agent 1â†’9 validation
            agent1_9_validated = agent1_9_result.get('validated', False)
            final_result.success = (
                final_result.data_integrity_score >= 0.7 and
                final_result.communication_quality >= 0.7 and
                final_result.integration_completeness >= 0.6 and
                agent1_9_validated  # CRITICAL: Agent 1â†’9 data flow must be validated
            )
            
            # Populate bridge status
            final_result.bridge_status = {
                bridge: info['status'] == 'active' 
                for bridge, info in self.active_bridges.items()
            }
            
            # Populate data flows
            final_result.data_flows = {
                'integrity_validation': integrity_result,
                'cross_reference_analysis': cross_ref_result,
                'agent1_agent9_flow': agent1_9_result,
                'integration_validation': integration_result
            }
            
            # Populate validation results
            final_result.validation_results = {
                'component_integrity': integrity_result.get('component_integrity', {}),
                'reference_integrity': cross_ref_result.get('reference_integrity', 0.0),
                'compatibility_matrix': integration_result.get('compatibility_matrix', {}),
                'platform_support': interop_result.get('supported_platforms', [])
            }
            
            # Collect all issues and warnings
            all_issues = []
            all_issues.extend(integrity_result.get('issues', []))
            all_issues.extend(cross_ref_result.get('unresolved_references', []))
            all_issues.extend(agent1_9_result.get('critical_issues', []))  # CRITICAL: Include Agent 1â†’9 issues
            all_issues.extend(integration_result.get('integration_issues', []))
            
            final_result.error_messages = all_issues
            
            # Generate communication logs
            final_result.communication_logs = [
                f"Established {len(self.active_bridges)} communication bridges",
                f"Validated {integrity_result.get('validated_components', 0)} components",
                f"Analyzed {len(cross_ref_result.get('reference_map', {}))} reference maps",
                f"Checked {len(integration_result.get('component_compatibility', {}))} compatibility pairs",
                f"Supported platforms: {', '.join(interop_result.get('supported_platforms', []))}"
            ]
            
            # Calculate metrics
            final_result.metrics = {
                'total_bridges': len(self.active_bridges),
                'active_bridges': sum(1 for info in self.active_bridges.values() if info['status'] == 'active'),
                'data_checksums_generated': len(self.data_checksums),
                'communication_channels': len(self.communication_channels),
                'reference_map_size': len(cross_ref_result.get('reference_map', {})),
                'compatibility_checks': len(integration_result.get('component_compatibility', {})),
                'platform_compatibility_average': interop_result.get('interoperability_score', 0.0)
            }
            
        except Exception as e:
            final_result.error_messages.append(f"Integration report generation error: {str(e)}")
            self.logger.error(f"Failed to generate integration report: {e}")
        
        return final_result
    
    # Helper methods
    def _calculate_component_integrity(self, agent_id: int, result: Dict[str, Any]) -> float:
        """Calculate data integrity score for a component"""
        integrity_factors = []
        
        # Check for required fields
        if 'status' in result:
            integrity_factors.append(0.3)
        
        if 'execution_time' in result:
            integrity_factors.append(0.2)
        
        if 'data' in result or any(key.endswith('_result') for key in result.keys()):
            integrity_factors.append(0.3)
        
        # Check data completeness
        data_size = len(str(result))
        if data_size > 1000:  # Substantial data present
            integrity_factors.append(0.2)
        elif data_size > 100:  # Minimal data present
            integrity_factors.append(0.1)
        
        return sum(integrity_factors)
    
    def _generate_data_checksum(self, data: Any) -> str:
        """Generate checksum for data validation"""
        data_str = json.dumps(data, sort_keys=True, default=str)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def _check_data_consistency(self, agent_id: int, result: Dict[str, Any], context: Dict[str, Any]) -> bool:
        """Check data consistency for a component"""
        try:
            # Basic consistency checks
            has_status = 'status' in result
            has_execution_time = 'execution_time' in result
            has_agent_info = 'agent_id' in result or 'agent_name' in result
            
            return has_status and has_execution_time and has_agent_info
        except:
            return False
    
    def _extract_component_references(self, agent_id: int, result: Dict[str, Any]) -> Dict[str, Any]:
        """Extract references from a component"""
        references = {
            'internal_references': [],
            'external_references': [],
            'data_dependencies': [],
            'function_calls': [],
            'variable_references': []
        }
        
        # Extract different types of references based on agent type
        data = result.get('data', {})
        
        # Look for function references
        if 'functions' in data:
            references['function_calls'].extend(data['functions'].keys())
        
        # Look for variable references
        if 'variables' in data:
            references['variable_references'].extend(data['variables'].keys())
        
        # Look for dependencies
        if 'dependencies' in result:
            references['data_dependencies'].extend(result['dependencies'])
        
        return references
    
    def _analyze_cross_component_references(self, reference_map: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze references across components"""
        cross_refs = {
            'shared_functions': [],
            'shared_variables': [],
            'component_dependencies': {},
            'reference_counts': {}
        }
        
        # Find shared elements across components
        all_functions = set()
        all_variables = set()
        
        for component, refs in reference_map.items():
            functions = refs.get('function_calls', [])
            variables = refs.get('variable_references', [])
            
            for func in functions:
                if func in all_functions:
                    if func not in cross_refs['shared_functions']:
                        cross_refs['shared_functions'].append(func)
                all_functions.add(func)
            
            for var in variables:
                if var in all_variables:
                    if var not in cross_refs['shared_variables']:
                        cross_refs['shared_variables'].append(var)
                all_variables.add(var)
            
            # Count references
            cross_refs['reference_counts'][component] = {
                'functions': len(functions),
                'variables': len(variables),
                'dependencies': len(refs.get('data_dependencies', []))
            }
        
        return cross_refs
    
    def _validate_reference_integrity(self, reference_map: Dict[str, Any], cross_component_refs: Dict[str, Any]) -> float:
        """Validate reference integrity across components"""
        if not reference_map:
            return 0.0
        
        total_references = sum(
            len(refs.get('function_calls', [])) + len(refs.get('variable_references', []))
            for refs in reference_map.values()
        )
        
        if total_references == 0:
            return 1.0  # No references to validate
        
        shared_references = len(cross_component_refs.get('shared_functions', [])) + \
                          len(cross_component_refs.get('shared_variables', []))
        
        # Calculate integrity as ratio of properly cross-referenced items
        return min(shared_references / (total_references * 0.1), 1.0)  # Expect 10% cross-references
    
    def _detect_unresolved_references(self, reference_map: Dict[str, Any]) -> List[str]:
        """Detect unresolved references"""
        unresolved = []
        
        # This is a simplified implementation
        # In a real system, we would check against symbol tables
        for component, refs in reference_map.items():
            for ref_type, ref_list in refs.items():
                if ref_type in ['function_calls', 'variable_references'] and len(ref_list) == 0:
                    unresolved.append(f"{component}: No {ref_type} found")
        
        return unresolved
    
    def _detect_circular_dependencies(self, reference_map: Dict[str, Any]) -> List[str]:
        """Detect circular dependencies"""
        # Simplified circular dependency detection
        circular_deps = []
        
        for component, refs in reference_map.items():
            dependencies = refs.get('data_dependencies', [])
            for dep in dependencies:
                if dep == component:
                    circular_deps.append(f"Self-dependency in {component}")
        
        return circular_deps
    
    def _perform_linking_analysis(self, reference_map: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """Perform linking analysis"""
        linking_analysis = {
            'linkable_components': [],
            'linking_requirements': {},
            'symbol_resolution': {},
            'linking_complexity': 0.0
        }
        
        # Analyze linking requirements for each component
        for component, refs in reference_map.items():
            if any(refs.values()):  # Has any references
                linking_analysis['linkable_components'].append(component)
                
                requirements = {
                    'external_symbols': len(refs.get('external_references', [])),
                    'function_imports': len(refs.get('function_calls', [])),
                    'variable_imports': len(refs.get('variable_references', []))
                }
                linking_analysis['linking_requirements'][component] = requirements
        
        # Calculate linking complexity
        total_refs = sum(
            sum(reqs.values()) for reqs in linking_analysis['linking_requirements'].values()
        )
        linking_analysis['linking_complexity'] = min(total_refs / 100.0, 1.0)  # Normalize to 0-1
        
        return linking_analysis
    
    def _check_component_compatibility(self, agent_a: int, agent_b: int, 
                                     result_a: Dict[str, Any], result_b: Dict[str, Any]) -> float:
        """Check compatibility between two components"""
        compatibility_factors = []
        
        # Check status compatibility
        status_a = result_a.get('status', 'unknown')
        status_b = result_b.get('status', 'unknown')
        
        if status_a == status_b and status_a == 'success':
            compatibility_factors.append(0.4)
        elif status_a == 'success' or status_b == 'success':
            compatibility_factors.append(0.2)
        
        # Check data format compatibility
        has_data_a = 'data' in result_a or any(key.endswith('_result') for key in result_a.keys())
        has_data_b = 'data' in result_b or any(key.endswith('_result') for key in result_b.keys())
        
        if has_data_a and has_data_b:
            compatibility_factors.append(0.3)
        elif has_data_a or has_data_b:
            compatibility_factors.append(0.15)
        
        # Check execution time reasonableness (compatibility in performance)
        time_a = result_a.get('execution_time', 0)
        time_b = result_b.get('execution_time', 0)
        
        if time_a > 0 and time_b > 0:
            time_ratio = min(time_a, time_b) / max(time_a, time_b)
            if time_ratio > 0.5:  # Similar execution times
                compatibility_factors.append(0.3)
            else:
                compatibility_factors.append(0.15)
        
        return sum(compatibility_factors)
    
    def _validate_data_flows(self, agent_results: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """Validate data flow paths between components"""
        data_flow_validation = {
            'flow_paths': {},
            'flow_integrity': 0.0,
            'broken_flows': [],
            'flow_efficiency': 0.0
        }
        
        # Trace data flow paths
        agents = sorted(agent_results.keys())
        flow_paths = []
        
        for i in range(len(agents) - 1):
            current_agent = agents[i]
            next_agent = agents[i + 1]
            
            flow_exists = self._check_data_flow_exists(
                agent_results[current_agent], agent_results[next_agent]
            )
            
            flow_path = f"agent_{current_agent}_to_agent_{next_agent}"
            data_flow_validation['flow_paths'][flow_path] = flow_exists
            flow_paths.append(flow_exists)
            
            if not flow_exists:
                data_flow_validation['broken_flows'].append(flow_path)
        
        # Calculate flow integrity
        if flow_paths:
            data_flow_validation['flow_integrity'] = sum(flow_paths) / len(flow_paths)
        
        return data_flow_validation
    
    def _check_data_flow_exists(self, result_a: Dict[str, Any], result_b: Dict[str, Any]) -> bool:
        """Check if data flow exists between two components"""
        # Simplified check - in real implementation would check actual data dependencies
        status_a = result_a.get('status', 'unknown')
        status_b = result_b.get('status', 'unknown')
        
        # Data flow exists if both components are successful
        return status_a == 'success' and status_b == 'success'
    
    def _check_protocol_compliance(self, agent_results: Dict[str, Any]) -> Dict[str, Any]:
        """Check protocol compliance across components"""
        compliance_result = {
            'compliant_components': [],
            'non_compliant_components': [],
            'compliance_score': 0.0,
            'protocol_violations': []
        }
        
        required_fields = ['agent_id', 'status', 'execution_time']
        
        for agent_id, result in agent_results.items():
            is_compliant = all(field in result for field in required_fields)
            
            if is_compliant:
                compliance_result['compliant_components'].append(f"agent_{agent_id}")
            else:
                compliance_result['non_compliant_components'].append(f"agent_{agent_id}")
                missing_fields = [field for field in required_fields if field not in result]
                compliance_result['protocol_violations'].append(
                    f"agent_{agent_id}: missing {missing_fields}"
                )
        
        total_components = len(agent_results)
        if total_components > 0:
            compliance_result['compliance_score'] = len(compliance_result['compliant_components']) / total_components
        
        return compliance_result
    
    def _validate_component_interfaces(self, agent_results: Dict[str, Any]) -> Dict[str, Any]:
        """Validate component interfaces"""
        interface_validation = {
            'valid_interfaces': [],
            'invalid_interfaces': [],
            'interface_score': 0.0,
            'interface_issues': []
        }
        
        for agent_id, result in agent_results.items():
            interface_valid = self._check_interface_validity(result)
            
            if interface_valid:
                interface_validation['valid_interfaces'].append(f"agent_{agent_id}")
            else:
                interface_validation['invalid_interfaces'].append(f"agent_{agent_id}")
                interface_validation['interface_issues'].append(f"agent_{agent_id}: invalid interface")
        
        total_components = len(agent_results)
        if total_components > 0:
            interface_validation['interface_score'] = len(interface_validation['valid_interfaces']) / total_components
        
        return interface_validation
    
    def _check_interface_validity(self, result: Dict[str, Any]) -> bool:
        """Check if component interface is valid"""
        # Basic interface validation
        has_required_fields = all(field in result for field in ['agent_id', 'status'])
        has_reasonable_structure = isinstance(result, dict) and len(result) > 0
        
        return has_required_fields and has_reasonable_structure
    
    def _generate_compatibility_matrix(self, agents: List[int], agent_results: Dict[str, Any]) -> Dict[str, Dict[str, float]]:
        """Generate compatibility matrix between all components"""
        matrix = {}
        
        for agent_a in agents:
            matrix[f"agent_{agent_a}"] = {}
            for agent_b in agents:
                if agent_a == agent_b:
                    matrix[f"agent_{agent_a}"][f"agent_{agent_b}"] = 1.0  # Self-compatibility
                else:
                    compatibility = self._check_component_compatibility(
                        agent_a, agent_b, agent_results[agent_a], agent_results[agent_b]
                    )
                    matrix[f"agent_{agent_a}"][f"agent_{agent_b}"] = compatibility
        
        return matrix
    
    def _check_platform_compatibility(self, platform: str, agent_results: Dict[str, Any], binary_info: Dict[str, Any]) -> float:
        """Check compatibility with specific platform"""
        platform_score = 0.5  # Base compatibility
        
        # Check binary architecture compatibility
        binary_arch = binary_info.get('architecture', 'unknown')
        
        if platform == 'windows':
            if 'x86' in binary_arch.lower() or 'pe' in binary_info.get('format', '').lower():
                platform_score += 0.3
        elif platform == 'linux':
            if 'x86' in binary_arch.lower() or 'elf' in binary_info.get('format', '').lower():
                platform_score += 0.3
        elif platform == 'macos':
            if 'x86' in binary_arch.lower() or 'mach' in binary_info.get('format', '').lower():
                platform_score += 0.3
        
        # Check agent compatibility with platform
        successful_agents = sum(1 for result in agent_results.values() if result.get('status') == 'success')
        total_agents = len(agent_results)
        
        if total_agents > 0:
            agent_success_rate = successful_agents / total_agents
            platform_score += agent_success_rate * 0.2
        
        return min(platform_score, 1.0)
    
    def _check_api_compatibility(self, agent_results: Dict[str, Any]) -> Dict[str, Any]:
        """Check API compatibility across components"""
        api_compatibility = {
            'compatible_apis': [],
            'incompatible_apis': [],
            'api_versions': {},
            'compatibility_score': 0.0
        }
        
        # Check for consistent API usage patterns
        for agent_id, result in agent_results.items():
            agent_name = f"agent_{agent_id}"
            
            # Check if agent follows expected API pattern
            follows_pattern = self._check_api_pattern(result)
            
            if follows_pattern:
                api_compatibility['compatible_apis'].append(agent_name)
            else:
                api_compatibility['incompatible_apis'].append(agent_name)
            
            # Extract version info if available
            if 'version' in result or 'api_version' in result:
                api_compatibility['api_versions'][agent_name] = result.get('version', result.get('api_version', 'unknown'))
        
        total_agents = len(agent_results)
        if total_agents > 0:
            api_compatibility['compatibility_score'] = len(api_compatibility['compatible_apis']) / total_agents
        
        return api_compatibility
    
    def _check_api_pattern(self, result: Dict[str, Any]) -> bool:
        """Check if result follows expected API pattern"""
        # Expected pattern: has status, execution_time, and some form of data/result
        required_pattern = ['status', 'execution_time']
        has_data = any(key.endswith('_result') or key == 'data' for key in result.keys())
        
        return all(field in result for field in required_pattern) and has_data
    
    def _check_data_format_compatibility(self, agent_results: Dict[str, Any]) -> Dict[str, Any]:
        """Check data format compatibility"""
        format_compatibility = {
            'supported_formats': set(),
            'format_consistency': 0.0,
            'format_issues': []
        }
        
        # Analyze data formats used by each agent
        for agent_id, result in agent_results.items():
            agent_name = f"agent_{agent_id}"
            
            # Check data types and formats
            if isinstance(result, dict):
                format_compatibility['supported_formats'].add('json/dict')
            
            # Check for specific data formats
            for key, value in result.items():
                if isinstance(value, str) and key.endswith('_path'):
                    format_compatibility['supported_formats'].add('file_path')
                elif isinstance(value, (int, float)) and key == 'execution_time':
                    format_compatibility['supported_formats'].add('numeric')
                elif isinstance(value, dict):
                    format_compatibility['supported_formats'].add('structured_data')
        
        # Calculate format consistency
        total_agents = len(agent_results)
        consistent_agents = sum(1 for result in agent_results.values() if isinstance(result, dict))
        
        if total_agents > 0:
            format_compatibility['format_consistency'] = consistent_agents / total_agents
        
        return format_compatibility
    
    def _check_version_compatibility(self, agent_results: Dict[str, Any]) -> Dict[str, Any]:
        """Check version compatibility across components"""
        version_compatibility = {
            'version_info': {},
            'compatible_versions': True,
            'version_conflicts': [],
            'version_coverage': 0.0
        }
        
        # Extract version information
        versions_found = 0
        for agent_id, result in agent_results.items():
            agent_name = f"agent_{agent_id}"
            
            if 'version' in result:
                version_compatibility['version_info'][agent_name] = result['version']
                versions_found += 1
            elif 'agent_version' in result:
                version_compatibility['version_info'][agent_name] = result['agent_version']
                versions_found += 1
        
        # Calculate version coverage
        total_agents = len(agent_results)
        if total_agents > 0:
            version_compatibility['version_coverage'] = versions_found / total_agents
        
        return version_compatibility
    def _create_failure_result(self, error_message: str, start_time: float, execution_time: float = None) -> 'AgentResult':
        """Create failure result using base class method with Link-specific data"""
        if execution_time is None:
            execution_time = time.time() - start_time
        
        # Create failure result using base class method
        base_result = super()._create_failure_result(error_message, start_time, execution_time)
        
        # Add Link-specific data
        base_result.data.update({
            'integration_result': IntegrationResult(),
            'phase': self.current_phase,
            'failure_point': self.current_phase
        })
        
        return base_result

# For backward compatibility
LinkAgent = Agent12_Link